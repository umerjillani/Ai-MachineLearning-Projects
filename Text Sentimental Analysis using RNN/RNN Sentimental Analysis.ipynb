{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNLf1XGJhJ/ncG9GWXpX0nf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install tensorflow\n","!pip install keras\n","!pip install keras-preprocessing"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zwZ7psPKSblu","executionInfo":{"status":"ok","timestamp":1729853513682,"user_tz":-300,"elapsed":7645,"user":{"displayName":"Zain Tech Tips","userId":"08994403296429920555"}},"outputId":"c5964c0a-da64-4838-c06e-e34afb9d70a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: keras==2.10.0 in /usr/local/lib/python3.10/dist-packages (2.10.0)\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","import numpy as np\n","import pandas as pd\n","from keras.models import Sequential\n","from keras.layers import Embedding, LSTM, Dense, Dropout\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"o8JUh8ELs25a"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ED6gWYgHR1f5","executionInfo":{"status":"ok","timestamp":1729854085979,"user_tz":-300,"elapsed":9776,"user":{"displayName":"Zain Tech Tips","userId":"08994403296429920555"}},"outputId":"09a22ad3-e9b9-4f8e-8f5c-c95e4a656e97"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 307ms/step - accuracy: 0.5000 - loss: 0.6937 - val_accuracy: 0.5000 - val_loss: 0.6920\n","Epoch 2/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.5000 - loss: 0.6902 - val_accuracy: 0.5000 - val_loss: 0.6921\n","Epoch 3/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.6843 - val_accuracy: 0.5000 - val_loss: 0.6925\n","Epoch 4/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.6756 - val_accuracy: 0.5000 - val_loss: 0.6928\n","Epoch 5/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.6742 - val_accuracy: 0.5000 - val_loss: 0.6925\n","Epoch 6/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.6655 - val_accuracy: 1.0000 - val_loss: 0.6926\n","Epoch 7/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.6608 - val_accuracy: 0.5000 - val_loss: 0.6925\n","Epoch 8/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.6713 - val_accuracy: 0.5000 - val_loss: 0.6932\n","Epoch 9/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.6572 - val_accuracy: 0.5000 - val_loss: 0.6931\n","Epoch 10/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.6434 - val_accuracy: 0.5000 - val_loss: 0.6927\n","Epoch 11/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 0.6375 - val_accuracy: 0.5000 - val_loss: 0.6921\n","Epoch 12/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.6142 - val_accuracy: 0.5000 - val_loss: 0.6920\n","Epoch 13/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.6110 - val_accuracy: 0.5000 - val_loss: 0.6910\n","Epoch 14/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.6155 - val_accuracy: 0.5000 - val_loss: 0.6911\n","Epoch 15/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.5912 - val_accuracy: 0.5000 - val_loss: 0.6898\n","Epoch 16/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.5827 - val_accuracy: 0.5000 - val_loss: 0.6890\n","Epoch 17/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.5724 - val_accuracy: 0.5000 - val_loss: 0.6863\n","Epoch 18/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.5187 - val_accuracy: 0.5000 - val_loss: 0.6811\n","Epoch 19/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.5004 - val_accuracy: 0.5000 - val_loss: 0.6728\n","Epoch 20/20\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.4458 - val_accuracy: 0.5000 - val_loss: 0.6669\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step - accuracy: 0.5000 - loss: 0.6669\n","Accuracy: 0.5000\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step\n","Sentence: \"I am really happy with this service!\" -> Sentiment: Positive\n","Sentence: \"This is a bad product.\" -> Sentiment: Negative\n"]}],"source":["data = {\n","    'sentence': [\n","        'I love this product!',\n","        'This is the worst experience I have ever had.',\n","        'Absolutely fantastic! Highly recommend it.',\n","        'Not worth the money.',\n","        'I am very satisfied with my purchase.',\n","        'This is terrible, I will never buy again.'\n","    ],\n","    'sentiment': [1, 0, 1, 0, 1, 0]  # 1 for positive, 0 for negative\n","}\n","\n","# Create DataFrame\n","df = pd.DataFrame(data)\n","\n","# Preprocess the data\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(df['sentence'])\n","sequences = tokenizer.texts_to_sequences(df['sentence'])\n","X = pad_sequences(sequences)\n","y = np.array(df['sentiment'])\n","\n","# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Build the RNN model with Dropout for regularization\n","model = Sequential()\n","model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=X.shape[1]))\n","model.add(LSTM(64, return_sequences=False))\n","model.add(Dropout(0.5))  # Add dropout layer for regularization\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the model with more epochs\n","model.fit(X_train, y_train, epochs=20, batch_size=2, validation_data=(X_test, y_test))\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(X_test, y_test)\n","print(f'Accuracy: {accuracy:.4f}')\n","\n","# Sample prediction\n","new_sentences = ['I am really happy with this service!', 'This is a bad product.']\n","new_sequences = tokenizer.texts_to_sequences(new_sentences)\n","new_X = pad_sequences(new_sequences, maxlen=X.shape[1])\n","predictions = model.predict(new_X)\n","predicted_sentiments = [1 if pred > 0.5 else 0 for pred in predictions]\n","\n","# Print predictions\n","for sentence, sentiment in zip(new_sentences, predicted_sentiments):\n","    print(f'Sentence: \"{sentence}\" -> Sentiment: {\"Positive\" if sentiment == 1 else \"Negative\"}')"]},{"cell_type":"code","source":[],"metadata":{"id":"fPOjdFOXR3Y5"},"execution_count":null,"outputs":[]}]}